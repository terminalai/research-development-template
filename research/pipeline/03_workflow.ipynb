{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp workflow\n",
    "%load_ext lab_black\n",
    "# nb_black if running in jupyter\n",
    "%load_ext autoreload\n",
    "# automatically reload python modules if there are changes in the\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "\n",
    "> Define static or dynamic workflow for automatically updating, training and deploying your ML model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***input:*** Workflow definition parameters\n",
    "\n",
    "***output:*** python or snakemake script for running the workflow\n",
    "\n",
    "***description:***\n",
    "\n",
    "While you are developing your ML application, you might prefer running the notebooks manually again and again.\n",
    "However, once you have deployed your model into production it becomes unpractical and compromizes scalability, modularity and the principle of ease of reproducibility.\n",
    "This happens regardless of what 'production' means to you - it might well be that you are just running the notebooks and viewing the results directly from them.\n",
    "Whatever you are doing, having a single command to run the whole workflow makes things so much easier.\n",
    "\n",
    "Workflow automation is also the part of the work where you'll probably notice a lot of bugs and nonrobustness in your notebooks.\n",
    "Probably a lot more than you anticipated, but try not to get frustrated! Debugging is big and important part of the work.\n",
    "\n",
    "In this notebook we explain alternatives for automating workflows, either as a static or dynamic. \n",
    "By following these examples (and further documentation on [papermill](https://papermill.readthedocs.io/) and [Snakemake](https://snakemake.readthedocs.io/)) you can parameterize your notebooks,\n",
    "run them automatically in a workflow, and even parameterize and automate the workflow definition.\n",
    "With this template, you can easily define very complex and versatile workflows, that are well documented in a notebook. \n",
    "\n",
    "We selected these tools for the template because they have stable community support, they are relatively easy to use and they fit our needs.\n",
    "There are also other tools for workflow management and orchestration that may better suite your needs. Feel free to use them.\n",
    "For more information, see for example this \n",
    "[comparison of workflow tools for Python](https://medium.com/@Minyus86/comparison-of-pipeline-workflow-packages-airflow-luigi-gokart-metaflow-kedro-pipelinex-5daf57c17e7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import papermill as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make direct derivations from the paramerters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run parameterized notebooks with papermill\n",
    "\n",
    "Papermill allows parameterizing and running notebooks from Python runtime with `papermill.execute_notebook(input, output, parameters)`.\n",
    "The `input` parameter is the notebook to be run. The `output` parameter is the filepath where copy of the executed notebook is saved with the results.\n",
    "This can be the same as the `input`, but you probably want to keep it separate - otherwise your version control may get messy. \n",
    "In this example executed notebooks are saved under `results/notebooks`. The `parameters` cell allows you to change settings of the notebooks.\n",
    "\n",
    "You may have noticed, that in the beginning of each notebook there is a cell with a comment `# This cell is tagged parameters`.\n",
    "The cell has been added `Parameters` tag. The template notebooks already contain the tag, \n",
    "but you can check [papermill documentation](https://papermill.readthedocs.io/en/latest/usage-parameterize.html) on how to do it on different notebook editors.\n",
    "In this cell, variables are assigned. What papermill does is, that any parameters given to the `execute_notebook` function are listed \n",
    "in a new cell right below the one tagged with parameters. The listed parameters will rewrite the default assignments.\n",
    "This is why you should not do anything else but simple assignments in the parameters cell.\n",
    "\n",
    "Let's show an example. Let's run the notebook 'model' with and without changing the 'seed'-parameter.\n",
    "Copies of the notebooks executed with different settings are stored in `results/notebooks`.\n",
    "The copies are saved with underscore prefix `_notebook.ipynb` so that they are ignored by nbdev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5cdd329d834e31979c1b682dcfec44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Executing:   0%|          | 0/43 [00:00<?, ?cell/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "# run model notebook with default parameters\n",
    "_ = pm.execute_notebook(\n",
    "    \"02_loss.ipynb\",\n",
    "    \"results/notebooks/_02_loss_default_params.ipynb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952645b1713a4277b7ba03ad5a156ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Executing:   0%|          | 0/44 [00:00<?, ?cell/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "# run model notebook with 'seed' -parameter changed from 0 to 1\n",
    "_ = pm.execute_notebook(\n",
    "    \"02_loss.ipynb\", \"results/notebooks/_02_loss_seed_1.ipynb\", parameters={\"seed\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now open the notebooks and compare the results. \n",
    "\n",
    "Now we could just define and run the complete workflow from this notebook: just define which notebooks to run, in which order and with which parameters.\n",
    "Then just run this notebook and the workflow is executed.\n",
    "We could even go further and parameterize this notebook to get parameterizable workflow execution.\n",
    "Then, we could use papermill in another application to run this notebook to execute the rest of the workflow.\n",
    "However, this approach has two main restrictions. The workflow execution script would not be included in this documentation, and it does not allow dynamic workflows because launching Snakemake from inside a Python runtime will cause all sorts of problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static executable workflow with papermill\n",
    "\n",
    "If we want to automatically run the workflow, we need to create and executable script to run it.\n",
    "We can define it in this notebook to include it in our documentation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile static_workflow.py\n",
    "# execute workflow of the example notebooks\n",
    "# to run the script, call python static_workflow.py workflow_setup.yaml\n",
    "# this file has been added to .gitignore\n",
    "# NOTE: use curly brackets only to format in global variables!\n",
    "# hint: you can include additional parameters with sys.argv\n",
    "\n",
    "# import relevant libraries\n",
    "import papermill as pm\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "# update modules before running just to be sure\n",
    "os.system('nbdev_build_lib')\n",
    "\n",
    "# run data notebook\n",
    "_ = pm.execute_notebook('00_data.ipynb', # input\n",
    "                        'results/notebooks/_00_data.ipynb', # output\n",
    "                       parameters = dict(seed = 0) # params (optional)\n",
    "                       )\n",
    "# run model notebook\n",
    "_ = pm.execute_notebook('01_model.ipynb',\n",
    "                        'results/notebooks/_01_model.ipynb')\n",
    "# run loss notebook\n",
    "_ = pm.execute_notebook('02_loss.ipynb',\n",
    "                       'results/_02_loss.ipynb')\n",
    "\n",
    "# optional (uncomment): make backup of the index and workflow notebooks:\n",
    "# os.system('cp {workflow} {save_notebooks_to}{workflow}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterized static executable workflow with papermill\n",
    "\n",
    "What if some of your input files change, or you would like to run your workflow with a slighly different setup?\n",
    "Just as we parameterized the components of the workflow, we might want to parameterize the workflow definition.\n",
    "You can either read parameters directly form sys.argv, python argparser or, like in this example, from a configuration file.\n",
    "\n",
    "Let's begin by defining the configuration file. We use the [yaml](https://yaml.org/) format, because it is easy to write and read by both humans and machines.\n",
    "The file is defined in this notebook and written directly into the `workflow_setup.yml` file. The config file is added to `.gitignore` - it is already defined in this notebook, we do not need double versioning. See [here](https://stackoverflow.com/questions/1773805/how-can-i-parse-a-yaml-file-in-python/1774043#1774043) how to use yaml with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting workflow_setup.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile workflow_setup.yml\n",
    "---\n",
    "notebooks: # workflow notebook setup\n",
    "    index: # name of notebook\n",
    "        notebook: index.ipynb # notebook file\n",
    "\n",
    "    data:\n",
    "        notebook: 00_data.ipynb\n",
    "        params: # notebook parameters\n",
    "            seed: 0\n",
    "    model:\n",
    "        notebook: 01_model.ipynb\n",
    "        params:\n",
    "            seed: 0\n",
    "    loss:\n",
    "        notebook: 02_loss.ipynb\n",
    "        params:\n",
    "            seed: 0\n",
    "utils: # general workflow settings\n",
    "    save_notebooks_to: results/notebooks/\n",
    "    notebook_save_prefix: _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look how the setup looks loaded as a python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'notebooks': {'index': {'notebook': 'index.ipynb'},\n  'data': {'notebook': '00_data.ipynb', 'params': {'seed': 0}},\n  'model': {'notebook': '01_model.ipynb', 'params': {'seed': 0}},\n  'loss': {'notebook': '02_loss.ipynb', 'params': {'seed': 0}}},\n 'utils': {'save_notebooks_to': 'results/notebooks/',\n  'notebook_save_prefix': '_'}}"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"workflow_setup.yml\", \"r\") as f:\n",
    "    setup_dict = yaml.load(f, Loader=yaml.Loader)\n",
    "setup_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the cell below to create the execution script. The code is not run in this notebook, but written in the file `static_workflow.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static_workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile static_workflow.py\n",
    "# execute workflow of the example notebooks\n",
    "# to run the script, call python static_workflow.py workflow_setup.yaml\n",
    "# this file has been added to .gitignore\n",
    "# NOTE: use curly brackets only to format in global variables!\n",
    "# hint: you can include additional parameters with sys.argv\n",
    "\n",
    "# import relevant libraries\n",
    "import papermill as pm\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "## parse arguments from workflow_setup.yaml\n",
    "configfilename = sys.argv[1]\n",
    "with open(configfilename, 'r') as f:\n",
    "    config = yaml.load(f, Loader = yaml.Loader)\n",
    "\n",
    "# variables\n",
    "notebooks = config['notebooks']\n",
    "data = notebooks['data']\n",
    "model = notebooks['model']\n",
    "loss = notebooks['loss']\n",
    "\n",
    "utils = config['utils']\n",
    "\n",
    "# update modules before running just to be sure\n",
    "os.system('nbdev_build_lib')\n",
    "\n",
    "# run data notebook\n",
    "_ = pm.execute_notebook(data['notebook'], # input\n",
    "                        utils['save_notebooks_to'] \\\n",
    "                        + utils['notebook_save_prefix'] \\\n",
    "                        + data['notebook'], # output\n",
    "                       parameters = data['params'] # params\n",
    "                       )\n",
    "# run model notebook\n",
    "_ = pm.execute_notebook(model['notebook'],\n",
    "                        utils['save_notebooks_to'] \\\n",
    "                        + utils['notebook_save_prefix'] \\\n",
    "                        + model['notebook'],\n",
    "                       parameters = model['params'])\n",
    "# run loss notebook\n",
    "_ = pm.execute_notebook(loss['notebook'],\n",
    "                        utils['save_notebooks_to'] \\\n",
    "                        + utils['notebook_save_prefix'] \\\n",
    "                        + loss['notebook'],\n",
    "                       parameters = loss['params'])\n",
    "\n",
    "# optional (uncomment): make backup of the index and workflow notebooks:\n",
    "# os.system('cp {workflow} {save_notebooks_to}{workflow}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you open the file `static_workflow.py`, you notice that the contents of curly brackets were replaced with the parameters of this notebook.\n",
    "\n",
    "Now, you can run the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh: 1: nbdev_build_lib: not found\n",
      "Executing:   0%|                                       | 0/58 [00:00<?, ?cell/s][IPKernelApp] WARNING | Unknown error in handling startup files:\n",
      "Executing: 100%|██████████████████████████████| 58/58 [00:19<00:00,  3.77cell/s]\n",
      "Executing:   0%|                                       | 0/45 [00:00<?, ?cell/s][IPKernelApp] WARNING | Unknown error in handling startup files:\n",
      "Executing: 100%|██████████████████████████████| 45/45 [00:18<00:00,  1.91cell/s]\n",
      "Executing:   0%|                                       | 0/44 [00:00<?, ?cell/s][IPKernelApp] WARNING | Unknown error in handling startup files:\n",
      "Executing: 100%|██████████████████████████████| 44/44 [00:13<00:00,  4.33cell/s]\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "# run this in your terminal to also run the nbdev_build_lib\n",
    "!python static_workflow.py workflow_setup.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can again make a visible copy of the hidden notebooks folder just like above (remember to delete it afterwards) and view the notebooks.\n",
    "You can change some of the notebook parameters and rerun the workflow to see how it effects the results.\n",
    "\n",
    "You see that static workflow definition is quite simple. In the script above, \n",
    "we did not define any inputs, outputs or the relation of the different steps.\n",
    "It's good to keep things that way, unless there is a reason not to.\n",
    "It might be that we have multiple, changing data sources, complex workflow structure,\n",
    "need for parallelization or other issues making it either difficult to hard-code\n",
    "the steps required in your workflow. Then, you might need a dynamic workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic executable workflow with Snakemake\n",
    "\n",
    "Snakemake is a tool that will automatically determine which steps to run based on inputs and outputs.\n",
    "It's like gnu make, but for Python: easy to read and write, but powerful.\n",
    "Unfortunaltely it is impossible to cover all the properties of the tool but see their documentation and internet discussion for ideas.\n",
    "Here we only cover a tiny portion of the possibilities of snakemake, but it can do very complex things.\n",
    "\n",
    "Snakemake executes the workflow as a rule based directed acyclic graph (DAG).\n",
    "Each workflow step is determined by a rule, which consists of inputs, outputs and execution of code.\n",
    "The inputs and outputs are files (data, config, source code, notebooks, images, tables etc.).\n",
    "The code executed can either be shell commands or Python, written directly into the Snakefile.\n",
    "\n",
    "Let's consider a workflow where we have two parallel rules 1a and 1b, and one consequtive rule 2.\n",
    "In addition we have rule all, that determines the whole workflow.\n",
    "The rules 1a and 1b only depend on their input. The rule 2 depends on outputs of both rules 1a and 1b.\n",
    "The rule 2 also has an additional input independent of other rules.\n",
    "We can visualize the workflow as follows:\n",
    "\n",
    "![Workflow visualization example](./visuals/snakemake_illustration_simple_workflow.png)\n",
    "\n",
    "Now, if we turned it into a Snakefile script, it would look something like this:\n",
    "\n",
    "    rule all: # used to determine the whole workflow\n",
    "        input:\n",
    "            output_2\n",
    "            \n",
    "    rule 1a: # parallel to rule 1b\n",
    "        input:\n",
    "            input_1a\n",
    "        output:\n",
    "            output_1a\n",
    "        run: # run python commands\n",
    "            # Python script to run rule 1a\n",
    "            \n",
    "     rule 1b: # parallel to rule 1a\n",
    "        input:\n",
    "            input_1b\n",
    "        output:\n",
    "            output_1b\n",
    "        shell: # you also run shell commands\n",
    "            # shell script to run rule 1b\n",
    "     \n",
    "     rule 2: # consequent to steps 1a and 1b\n",
    "        input:\n",
    "            output_1a, # depends on rule 1a\n",
    "            output_1b, # depends on rule 1b\n",
    "            input_2 # independent input\n",
    "        output:\n",
    "            output_2\n",
    "        run:\n",
    "            # Python script to run rule2\n",
    "\n",
    "Snakemake can be either used to run a single rule, or the complete workflow based on rule all.\n",
    "Based on the inputs and outputs, snakemake will determine which other rules will then need to be executed.\n",
    "\n",
    "In our example, the notebooks consist the nodes of the DAG graph. Based on the changes in input and output files since the last execution, \n",
    "Snakemake determines which steps need to be run. If no changes are observed, snakemake does not do anything. \n",
    "\n",
    "For example if we change the input of rule 1b, rules 1b and 2 are executed. If we make a change to the input of rule 2, only the rule 2 will be executed.\n",
    "If we want to rerun all steps without changin anything, you can just touch the inputs of the independent rules `touch input_1a input_1b`.\n",
    "\n",
    "The following script will be written into a Snakefile, that you can run to execute the workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Snakefile\n",
    "# import relevant libraries\n",
    "import papermill as pm\n",
    "import os\n",
    "\n",
    "# determine global variables, wildcards etc.\n",
    "\n",
    "# all: final output of the workflow\n",
    "rule all:\n",
    "    input:\n",
    "        'results/LogisticRegressionClassifier.pkl' # trained model\n",
    "        \n",
    "# data\n",
    "rule data:\n",
    "    input:\n",
    "        '00_data.ipynb' # every notebook is it's own input\n",
    "        # if we had input files they should be listed here, separated with comma ,\n",
    "    output: # Snakemake checks that after running these files are created / updated\n",
    "        'data/preprocessed_data/dataset_clean_switzerland_cleveland.csv',  # clean dataset\n",
    "        'data/preprocessed_data/dataset_toy_switzerland_cleveland.csv', # toy dataset\n",
    "        'ml_project_template/data.py', # plot functions\n",
    "        'results/notebooks/_00_data.ipynb' # copy of the executed notebook\n",
    "    run:\n",
    "        # run notebook with papermill\n",
    "        _ = pm.execute_notebook('00_data.ipynb', # input\n",
    "                                'results/notebooks/_00_data.ipynb', # output\n",
    "                               parameters = {'seed':0}) # params (optional)\n",
    "        os.system('nbdev_build_lib --fname 00_data.ipynb') # build data.py\n",
    "\n",
    "rule model:\n",
    "    input:\n",
    "        '01_model.ipynb',\n",
    "        'data/preprocessed_data/dataset_toy_switzerland_cleveland.csv',\n",
    "    output:\n",
    "        'ml_project_template/model.py', # model class\n",
    "        'results/notebooks/_01_model.ipynb'\n",
    "    run:\n",
    "        _ = pm.execute_notebook('01_model.ipynb', # we could also use '{input[0]}'\n",
    "                                'results/notebooks/_01_model.ipynb') # we could also use '{output[1]}'\n",
    "        os.system('nbdev_build_lib --fname 01_model.ipynb')\n",
    "        \n",
    "rule loss:\n",
    "    input:\n",
    "        '02_loss.ipynb',\n",
    "        'data/preprocessed_data/dataset_clean_switzerland_cleveland.csv',\n",
    "        'ml_project_template/data.py',\n",
    "        'ml_project_template/model.py'\n",
    "    output:\n",
    "        'results/LogisticRegressionClassifier.pkl', # trained model\n",
    "        'results/notebooks/_02_loss.ipynb'\n",
    "    run:\n",
    "        _ = pm.execute_notebook('02_loss.ipynb',\n",
    "                               'results/notebooks/_02_loss.ipynb')\n",
    "        os.system('nbdev_build_lib --fname 02_loss.ipynb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to notice is that Snakemake should be used from terminal directly, not from a Python script or notebook.\n",
    "\n",
    "To run Snakemake, call:\n",
    "    \n",
    "    snakemake -n # dry-run snakemake to check what would be done\n",
    "    snakemake --jobs 1 # run workflow\n",
    "\n",
    "The --jobs parameter (you can also use -j) is the maximum number of CPU cores to use with the pipeline (local/cluster/cloud cores).\n",
    "Usually 1 is enough with simple pipelines, since our primary workflow step is a notebook, and most Python operations are difficult to parallelize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterized dynamic executable workflow with Snakemake\n",
    "\n",
    "Again, we might want to parameterize the notebook for scalability.\n",
    "\n",
    "For simple parameterization, use rule parameters:\n",
    "    \n",
    "    rule model:\n",
    "        input:\n",
    "        output:\n",
    "        parameters: seed = 0\n",
    "        run:\n",
    "            _ = pm.execute_notebook(...,\n",
    "                                    ...,\n",
    "                               parameters = {seed: parameters.seed})\n",
    "\n",
    "For complex parameterization, including the parameterization of inputs and outputs,\n",
    "consider using a config file. Snakemake can automatically load  json or yaml file to python dictionary:\n",
    "\n",
    "    ## in config.yml:\n",
    "    rules:\n",
    "        data:\n",
    "            ...\n",
    "        model:\n",
    "            input:\n",
    "                - 01_model.ipynb\n",
    "                - data/preprocessed_data/dataset_toy_switzerland_cleveland.csv\n",
    "            parameters:\n",
    "                seed: 0\n",
    "            ...\n",
    "        ...\n",
    "    \n",
    "    \n",
    "    ## in Snakefile:\n",
    "    \n",
    "    # automatically load json or yaml file to python dictionary 'config'\n",
    "    configfile: config.yml\n",
    "    \n",
    "    rule model:\n",
    "        input:\n",
    "            config['rules']['model']['input']\n",
    "        output:\n",
    "            ...\n",
    "        run:\n",
    "            run:\n",
    "            _ = pm.execute_notebook(...,\n",
    "                                    ...,\n",
    "                               parameters = {seed: config['rules']['model']['parameters']['seed']})\n",
    "\n",
    "\n",
    "For more information, see [Snakemake documentation](https://snakemake.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Use this notebook to create an API\n",
    "\n",
    "If you just want to create conventional Python application, you can use this notebook to create main.py so that you can just run your module as python application.\n",
    "Then, you should change the name of this notebook and the default_exp location to main. This requires that you define and export all functions and classes to modules so that they can be used elsewhere. This is a bit messy, and deminishes many of the benefits of notebook development because instead of just running the notebooks you already created, you have to redefine all the steps required all over again. However, sometimes this might be what you want to do, so we wanted to mention it here.\n",
    "\n",
    "Pseudo example of how to define main.py in a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'False'\n"
     ]
    }
   ],
   "source": [
    "%%script False\n",
    "## remove this line and the line above if you want to run and export the code (note that it needs editing to work)\n",
    "### export main\n",
    "# remove the two extra # in the line above\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import data, model, loss\n",
    "\n",
    "def get_input():\n",
    "    \"\"\"\n",
    "    get input from user\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def main(filename):\n",
    "    \"\"\"\n",
    "    run main loop\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    X, y = data.load(filename)\n",
    "    # initialize model, fit, optimize\n",
    "    m = model.LogisticRegressionClassifier(X, y).fit().optimize()\n",
    "    # main loop\n",
    "    input_data = get_input()\n",
    "    while(input_data):\n",
    "        if input_data[0] == 'predict':\n",
    "            # predict on input\n",
    "            print(m.predict(input_data[1]))\n",
    "        else:\n",
    "            # do other things\n",
    "            pass\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    run with: python ml_project_template filename\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    filename = sys.argv[1]\n",
    "    main(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'False'\n"
     ]
    }
   ],
   "source": [
    "%%script False\n",
    "## remove this line and the line above if you want to run the code (note that it needs editing to work)\n",
    "## test the main loop in this notebook and interact with the application\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (templateenv)",
   "language": "python",
   "name": "templateenv_py3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
